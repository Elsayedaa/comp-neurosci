{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3A: Data and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data as a random variables\n",
    "\n",
    "Most statistics operates under the assumption that any observed data are actual samples drawn from some (to be learned) model of the world.\n",
    "\n",
    "In the standard approach, these models are simply probability distributions, with parameters that govern the behavior of the model (i.e., what we can expect they will produce)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainty\n",
    "\n",
    "If the observed data samples are simply random draws from a probability distribution, then the level of uncertainty will decrease as we gain more data samples.\n",
    "\n",
    "Statistical inference involves figuring out what model (e.g., probability distribution, but we will be building more complicated models later in the course) and parameters generated the data. \n",
    "\n",
    "Let's spend some time trying to perform this task by hand..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "import ipywidgets as widgets      # interactive widgets\n",
    "from ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "\n",
    "# import the distributions wrapped from scipy\n",
    "from tools import dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "with open('data/random_data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# tell us something about the data\n",
    "for i,d in enumerate(data):\n",
    "    print('Dataset %d has %d samples' % (i, len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# mapper between dist name and object\n",
    "dist_dict = {'Beta': dists.beta,\n",
    "             'Exponential': dists.exp,\n",
    "             'Gamma': dists.gamma,\n",
    "             'Normal': dists.normal,\n",
    "             'Uniform': dists.uniform}\n",
    "\n",
    "# Dropdown widget to pick datasets\n",
    "ds_ind = widgets.Dropdown(options=range(len(data)),\n",
    "                          description=\"Dataset\")\n",
    "\n",
    "# Checkbox for whether to show log likelihood\n",
    "like_check = widgets.Checkbox(description='Show Log Likelihood', \n",
    "                              value=False)\n",
    "\n",
    "# Checkbox for whether to show stem plot\n",
    "stem_check = widgets.Checkbox(description='Show Stem Plot', \n",
    "                              value=False)\n",
    "\n",
    "# set up the distributions tab\n",
    "dist_tab = widgets.Tab(allow_none=False)\n",
    "children = []\n",
    "titles = []\n",
    "\n",
    "# beta params\n",
    "beta_alpha = widgets.FloatText(value=.5,\n",
    "                               description='alpha')\n",
    "beta_beta = widgets.FloatText(value=.5,\n",
    "                              description='beta')\n",
    "children.append(widgets.HBox([beta_alpha, beta_beta]))\n",
    "titles.append('Beta')\n",
    "\n",
    "# exp params\n",
    "exp_lam = widgets.FloatText(value=5.0,\n",
    "                            description='lam')\n",
    "children.append(widgets.HBox([exp_lam]))\n",
    "titles.append('Exponential')\n",
    "\n",
    "# gamma params\n",
    "gamma_alpha = widgets.FloatText(value=.5,\n",
    "                                description='alpha')\n",
    "gamma_beta = widgets.FloatText(value=.5,\n",
    "                              description='beta')\n",
    "children.append(widgets.HBox([gamma_alpha, gamma_beta]))\n",
    "titles.append('Gamma')\n",
    "\n",
    "# normal params\n",
    "normal_mean = widgets.FloatText(value=0.0,\n",
    "                                description='mean')\n",
    "normal_std = widgets.FloatText(value=1.0,\n",
    "                               description='std')\n",
    "children.append(widgets.HBox([normal_mean, normal_std]))\n",
    "titles.append('Normal')\n",
    "\n",
    "# uniform params\n",
    "uniform_lower = widgets.FloatText(value=0.0,\n",
    "                                  description='lower')\n",
    "uniform_upper = widgets.FloatText(value=1.0,\n",
    "                                  description='upper')\n",
    "children.append(widgets.HBox([uniform_lower, uniform_upper]))\n",
    "titles.append('Uniform')\n",
    "\n",
    "# add all the children and set the tab titles\n",
    "dist_tab.children = children\n",
    "for i in range(len(titles)):\n",
    "    dist_tab.set_title(i, titles[i])\n",
    "\n",
    "# set the full user interface\n",
    "ui = widgets.VBox([widgets.HBox([ds_ind, like_check, stem_check]), dist_tab])\n",
    "\n",
    "# define plotting function\n",
    "def plot_data_and_dist(*vals, **kwargs):\n",
    "    # first plot the data\n",
    "    dat = data[ds_ind.value]\n",
    "    plt.hist(dat, bins='auto', density=True, alpha=.5);\n",
    "    \n",
    "    # now plot the pdf of the dist\n",
    "    npoints = 100\n",
    "    \n",
    "    # add support for 10% of the data range on either side\n",
    "    support = (dat.min() - np.ptp(dat)*.1,\n",
    "               dat.max() + np.ptp(dat)*.1)\n",
    "    x = np.linspace(support[0], support[1], npoints)\n",
    "    \n",
    "    # get the selected dist and params\n",
    "    params = {c.description: c.value for c in \n",
    "              dist_tab.children[dist_tab.selected_index].children}\n",
    "    dist = dist_dict[dist_tab.get_title(dist_tab.selected_index)](**params)\n",
    "    \n",
    "    # calculate the pdf\n",
    "    pdf = dist.pdf(x)\n",
    "            \n",
    "    # plot the pdf and add labels\n",
    "    plt.plot(x, pdf, lw=3)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability')\n",
    "    if like_check.value:\n",
    "        # calculate the log like\n",
    "        log_like = np.log(dist.pdf(dat)).sum()\n",
    "\n",
    "        # add it to the plot with some formatting\n",
    "        plt.title('Log Like: {:3.4f}'.format(log_like))\n",
    "    \n",
    "    if stem_check.value:\n",
    "        # include the stem plot\n",
    "        plt.stem(dat, dist.pdf(dat), 'g')\n",
    "\n",
    "        \n",
    "# set up triggers for updating the plot\n",
    "out = widgets.interactive_output(plot_data_and_dist, \n",
    "                                 {'ds_ind': ds_ind,\n",
    "                                  'like_check': like_check,\n",
    "                                  'stem_check': stem_check,\n",
    "                                  'beta_alpha': beta_alpha,\n",
    "                                  'beta_beta': beta_beta,\n",
    "                                  'exp_lam': exp_lam,\n",
    "                                  'gamma_alpha': gamma_alpha,\n",
    "                                  'gamma_beta': gamma_beta,\n",
    "                                  'normal_mean': normal_mean,\n",
    "                                  'normal_std': normal_std,\n",
    "                                  'uniform_lower': uniform_lower,\n",
    "                                  'uniform_upper': uniform_upper,\n",
    "                                 })\n",
    "\n",
    "# wrapper for tab change\n",
    "# required b/c selecting tabs can't trigger plots like other widgets\n",
    "def tab_change(*args, **kwargs):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        plot_data_and_dist()\n",
    "        show_inline_matplotlib_plots()\n",
    "dist_tab.observe(tab_change, 'selected_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive data fitting (part 2)\n",
    "\n",
    "The widget below (h/t Per Sederberg) shows the histogram of a randomly generated distribution in blue, overlaid with the PDF from one of five different distribution types in orange. Select one of the datasets and try to figure out what distribution it was generated from. You'll likely need to adjust the parameters of the distribution.\n",
    "\n",
    "Refer back to Notebook 2 and/or Wikipedia if you want to remind yourself of what the different distributions look like and how they change shape with their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show everything\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What dataset did you pick? What distribution and parameters best fit the observations by eye?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Likelihood calculation\n",
    "\n",
    "We don't have to do this by eye. Because our model has a probability density function (PDF), we can calculate the likelihood of observing the data given the model and parameters.\n",
    "\n",
    "For any given model and parameters, you can determine the probability of having observed any individual data point by evaluating the PDF at the value of that data point (*Turn on the Stem Plot.*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "The goal then becomes to maximize the likelihood of observing the data given a model and parameters:\n",
    "\n",
    "$$P(D \\mid \\theta, M)$$\n",
    "\n",
    "As long as the data points are all independent, the likelihood of observing all of them is the product of all the probabilities.\n",
    "\n",
    "$$P(D \\mid \\theta, M) = \\prod_i p(d_i \\mid \\theta, M)$$\n",
    "\n",
    "It is more efficient and computationally tractable to maximize the log of $P(D|\\theta,M)$, so we typically convert the likelihood into a sum of log likelihoods.\n",
    "\n",
    "$$ \\log P(D \\mid \\theta, M) = \\sum_i \\log p(d_i \\mid \\theta, M)$$\n",
    "\n",
    "**Q** Turn on the Log Likelihood checkbox above and see if you can do better. Note that you'll get a numerical error if any of the data lie outside the support of the distribution, but this will go away once you fix that issue. In the cell below, enter your best estimates of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automated optimization\n",
    "\n",
    "Many approaches have been developed for searching parameter spaces to find the parameters that generate the maximum or minimum value of a function.\n",
    "\n",
    "### Nelder--Mead Simplex\n",
    "\n",
    "One very popular algorithm is the Nelder--Mead simplex.\n",
    "\n",
    "It involves growing and shrinking a simplex (a generalization of a triangle to multiple dimensions) to search the parameter space efficiently to minimize a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Nelder-Mead_Himmelblau.gif/640px-Nelder-Mead_Himmelblau.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# scipy includes lots of optimization methods\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function\n",
    "def like_fun(params, *args):\n",
    "    # pull the model and dat out of the args\n",
    "    model = args[0]\n",
    "    dat = args[1]\n",
    "    \n",
    "    # instantiate the model with the params\n",
    "    dist = model(*params)\n",
    "    \n",
    "    # calc the log like\n",
    "    log_like = np.log(dist.pdf(dat)).sum()\n",
    "    if np.isnan(log_like):\n",
    "        log_like = -np.inf\n",
    "    \n",
    "    # return the negative of it to minimize\n",
    "    return -log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the best-fitting params\n",
    "\n",
    "Execute the following code block to run Nelder-Mead on your dataset, using your chosen model from the widget above. Hopefully, in some small amount of time, the algorithm will converge on a solution, and you'll get a printout. See if you can figure out what the best parameter estimates are from the output.\n",
    "\n",
    "Notice that each time you run the code below, a new starting value for the optimization is generated. Try running the optimization a few times and see if you get similar answers. Why is it important to do this?\n",
    "\n",
    "If you're not sure if you've chosen the right PDF, go back to the widget, pick a different one, and rerun the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the distribution and data from the UI above\n",
    "dist_name = dist_tab.get_title(dist_tab.selected_index)\n",
    "model = dist_dict[dist_name]\n",
    "dat = data[ds_ind.value]\n",
    "\n",
    "# set the bounds for the distribution\n",
    "bound_dict = {'Beta': [(0, 10), (0, 10)],\n",
    "              'Exponential': [(0, 20)],\n",
    "              'Gamma': [(0, 10), (0, 10)],\n",
    "              'Normal': [(-10, 10), (0, 10)],\n",
    "              'Uniform': [(-10, 10), (-10, 10)]}\n",
    "bounds = bound_dict[dist_name]\n",
    "\n",
    "# generate a random starting point based on the bounds\n",
    "# NB: it's possible to generate invalid starting points\n",
    "x0 = [dists.uniform(*b).rvs() for b in bounds]\n",
    "\n",
    "# print some information about the distribution and starting values\n",
    "print('Dataset:', ds_ind.value)\n",
    "print('Distribution:', dist_name)\n",
    "print('Starting value:', x0)\n",
    "print()\n",
    "\n",
    "# run the optimizer\n",
    "# NOTE, not all methods make use of the bounds method\n",
    "res = opt.minimize(like_fun, x0, args=(model, dat), \n",
    "                   #bounds=bounds,\n",
    "                   #method='L-BFGS-B',\n",
    "                   #method='BFGS',\n",
    "                   method='Nelder-Mead',\n",
    "                   #method='TNC'\n",
    "                  )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize the fit\n",
    "\n",
    "Let's visualize the likelihood as a 2D function of the parameters and see how our fit looks. If you're far away from where you think you should be, try picking some different values in the widget and/or re-running the optimization.\n",
    "\n",
    "Note that this will only work for PDFs that have two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the grid of points to evaluate\n",
    "x = np.linspace(bounds[0][0], bounds[0][1], 100)\n",
    "y = np.linspace(bounds[1][0], bounds[1][1], 100)\n",
    "xx, yy = np.meshgrid(x, y, sparse=True)\n",
    "\n",
    "# evaluate the likelihood\n",
    "z = np.log(model(xx, yy).pdf(dat[:, np.newaxis, np.newaxis])).sum(0)\n",
    "z = np.exp(z)\n",
    "\n",
    "# plot the contour and the best-fit value\n",
    "plt.contourf(x, y, z, 100)\n",
    "plt.plot(res.x[0], res.x[1], 'x', markersize=12, color='red')\n",
    "plt.colorbar()\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Is your estimate at the peak of the likelihood function? Are some parameters constrained more or less than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing model fit\n",
    "\n",
    "An important question you need to address when using a statistical model is how well your model fits the data.\n",
    "\n",
    "Your fit can be bad for any number of reasons, but one rather common one is that your model is wrong.\n",
    "\n",
    "If your model is really wrong, then the parameter estimates are not worth much.\n",
    "\n",
    "However, because all models are wrong (see Box's Dictum), you can only compare models to see which are less wrong.\n",
    "\n",
    "How do we do this?\n",
    "\n",
    "We could simply compare the maximum likelihoods between the models, but that doesn't take into account the complexity of each model and amount of data. \n",
    "\n",
    "One, more principled, approach is Bayesian Information Criterion (BIC):\n",
    "\n",
    "$$BIC = \\text{ln}(n)k - 2\\text{ln}(\\hat{L}),$$\n",
    "\n",
    "where $n$ is the number of data points, $k$ is the number of parameters, and $\\hat{L}$ is the maximum likelihood value of the model $M$.\n",
    "\n",
    "***SMALLER BIC values are better!!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model fit with BIC\n",
    "# recall that the optimization returns the negative log likelihood\n",
    "n = len(dat)\n",
    "k = len(res.x)\n",
    "L = -res.fun\n",
    "bic = np.log(n)*k - 2*(L)\n",
    "print('BIC:', bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "But how do we decide if one model is better than the other? We can compare BIC values between models, turning them into a Bayes Factor!\n",
    "\n",
    "$$BF_{01} = exp((BIC_1 - BIC_0)/2)$$\n",
    "\n",
    "This is interpreted with the help of the following guidelines:\n",
    "\n",
    "| Bayes Factor | Evidence |\n",
    "|--------------|----------|\n",
    "| 1--3         | Weak     |\n",
    "| 3--20        | Positive |\n",
    "| 20--150      | Strong   |\n",
    "| >150         | Very Strong | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Copy your first BIC value into the cell below, assigning it to `bic_0`. Now try fitting your data to another  distribution functions using the widget above. Assign the second BIC value to `bic_1` below and the execute the cell to calculate the Bayes Factor. Then, in the text cell below, describe which two PDFs you considered, the maximum likelihood *parameter estimates*, and which model fit the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc Bayes Factor\n",
    "# (enter numbers from the BIC assessments above)\n",
    "bic_0 = -33.93\n",
    "bic_1 = -31.07\n",
    "np.exp((bic_1 - bic_0)/2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## t-test example\n",
    "\n",
    "We now have all the tools necessary to perform statistical inference (though we will improve on all these approaches in the coming weeks). You can:\n",
    "\n",
    "- Use optimization techniques to identify the parameters that give rise to the maximum likelihood of observing the data given the model\n",
    "- Assess model fit\n",
    "- Compare models to guide model selection\n",
    "\n",
    "Let's try a simple example of performing a t-test via model comparison approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You're hopefully quite familiar with how you can use a t-test to determine if a the mean of a sample is significantly different from zero, or to determine if two samples have different means. But what's going on under the hood?\n",
    "\n",
    "Let's consider the one-sample t-test. You run a t-test and get a $p$ value, which tells you the probability that you'd get your observations if the data were from a normal distribution with mean $\\mu = 0$ (the null hypothesis).\n",
    "\n",
    "The model comparison standpoint is similar, but what you're testing is whether there's more evidence for a model where $\\mu \\neq 0$ than for one in which $\\mu = 0$. We can make this comparison using Bayes Factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate some data that may or may not be significantly different from zero.\n",
    "# you can play around with the sample size, mean, and std deviation later\n",
    "A = dists.normal(.3, .5).rvs(10)\n",
    "\n",
    "# plot it\n",
    "plt.hist(A, bins='auto', density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard t-test\n",
    "\n",
    "First we'll perform a standard one-sample t-test on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a one-sample t-test\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.ttest_1samp(A, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a Student's t model\n",
    "\n",
    "Next we fit the full model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function\n",
    "def students_like(params, *args):\n",
    "    # pull the model and dat out of the args\n",
    "    dat = args[0]\n",
    "    df = len(dat) - 1\n",
    "    \n",
    "    # instantiate the model with the params, \n",
    "    # the df is determined from the data\n",
    "    dist = dists.students_t(params[0], params[1], df)\n",
    "    \n",
    "    # calc the log like\n",
    "    log_like = np.log(dist.pdf(dat)).sum()\n",
    "    if np.isnan(log_like):\n",
    "        log_like = -np.inf\n",
    "    \n",
    "    # return the negative of it to minimize\n",
    "    return -log_like\n",
    "\n",
    "# pick an central starting point\n",
    "x0 = [0.0, 1.0]\n",
    "\n",
    "# run the optimization\n",
    "res = opt.minimize(students_like, x0, args=(A,), \n",
    "                   #bounds=bounds,\n",
    "                   #method='L-BFGS-B',\n",
    "                   #method='BFGS',\n",
    "                   method='Nelder-Mead',\n",
    "                   #method='TNC'\n",
    "                  )\n",
    "print(res)\n",
    "\n",
    "# calculate the BIC for this model and save it\n",
    "n = len(A)\n",
    "k = len(res.x)\n",
    "L = -res.fun\n",
    "bic_1 = np.log(n)*k - 2*(L)\n",
    "print('BIC:', bic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a null hypothesis model\n",
    "\n",
    "Now we fit a model representing the null hypothesis that the mean of the data is actually 0.0. \n",
    "\n",
    "Note how we simply fix the mean of the Student's t distribution to zero, but still fit the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function\n",
    "def students_null_like(params, *args):\n",
    "    # pull the model and dat out of the args\n",
    "    dat = args[0]\n",
    "    df = len(dat) - 1\n",
    "    \n",
    "    # instantiate the model with the params\n",
    "    # mean is fixed at zero and the df is determined from the data\n",
    "    dist = dists.students_t(0.0, params[0], df)\n",
    "    \n",
    "    # calc the log like\n",
    "    log_like = np.log(dist.pdf(dat)).sum()\n",
    "    if np.isnan(log_like):\n",
    "        log_like = -np.inf\n",
    "    \n",
    "    # return the negative of it to minimize\n",
    "    return -log_like\n",
    "\n",
    "# start at same point (though mean is fixed at zero)\n",
    "x0 = [1.0]\n",
    "\n",
    "# run the optimization\n",
    "res = opt.minimize(students_null_like, x0, args=(A,), \n",
    "                   #bounds=bounds,\n",
    "                   #method='L-BFGS-B',\n",
    "                   #method='BFGS',\n",
    "                   method='Nelder-Mead',\n",
    "                   #method='TNC'\n",
    "                  )\n",
    "print(res)\n",
    "\n",
    "# calculate and print the BIC\n",
    "n = len(A)\n",
    "k = len(res.x)\n",
    "L = -res.fun\n",
    "bic_0 = np.log(n)*k - 2*(L)\n",
    "print('BIC:', bic_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Comparison\n",
    "\n",
    "Now that we have the BIC values for each model, we can use the Bayes Factor to determine whether the full model is preferred to the null model. \n",
    "\n",
    "We want a big number here. If it's less than 1.0 then there is no evidence that the alternative/full model should be preferred to the null model (i.e., the mean of the distribution is not different from 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Bayes Factor\n",
    "bf = np.exp((bic_0 - bic_1)/2.)\n",
    "print('Bayes Factor:', bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: In the cell that generates the observations, try three different values for the mean, standard deviation OR sample size. Then run the model-fitting code for each case. In the cell below, compare and contrast how changing these values affect the Bayes Factor and the p-value in the standard t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3B: Data Input/Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computational neuroscience often involves a lot of data. In this lesson, you'll learn about:\n",
    "\n",
    "- different kinds of data, how they are stored on disk\n",
    "- how to read data from the disk into your program\n",
    "- how to store data from your program to the disk\n",
    "\n",
    "We'll consider three kinds of data:\n",
    "\n",
    "- [time series](#Time-series-data)\n",
    "- [point processes](#Point-processes)\n",
    "- [structured records](#Structured-records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time series data\n",
    "\n",
    "Typically represented as an **array** of measurements: \n",
    "\n",
    "$$\\mathbf{x} = \\{x_0, x_1, \\ldots, x_N\\}$$\n",
    "\n",
    "Can be **multichannel** if more than one measurement taken at a time. Each time point is now a **vector** ($\\vec{x}$):\n",
    "\n",
    "$$\\mathbf{X} = \\{\\vec{x}_0, \\vec{x}_1, \\ldots, \\vec{x}_N\\}$$\n",
    "\n",
    "Multichannel time series are represented as two-dimensional arrays. One dimension correponds to time and the other to the component of the measurement vector.\n",
    "\n",
    "Note that the \"channels\" can be repeated **trials** rather than simultaneous measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time series in Python\n",
    "\n",
    "We use `numpy` arrays to store single- and multichannel time series in Python. Let's look at an example using some Gaussian white noise.\n",
    "\n",
    "Gaussian noise is drawn from a normal distribution, and it's called white noise because it has equal power at all frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)                  # set random seed so we all get the same results\n",
    "x = np.random.randn(1000)          # generate 100 random WN samples\n",
    "\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can determine the number of elements in a 1D array using the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of points in x is:\", len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q**: Do you recall from the last exercise how to access subsets of a numpy array? In the code cell below, write an expression to evaluate the mean of the first 100 samples of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy (and Python in general) supports **negative indexing**, which means that negative indices are interpreted as referencing elements from the **end** of the array. The following expression gives us the mean of the last 100 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(x[-100:1000])\n",
    "# you can leave out the second index in the slice if it refers to the end of the array, so this is equivalent:\n",
    "np.mean(x[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multichannel time series\n",
    "\n",
    "For multichannel data, the array has two dimensions. There is a (weak) convention that the first dimension of the array represents time. That means each column represents a separate channel.\n",
    "\n",
    "Here is an example of a 3-channel array, again using Gaussian white noise. I've added some correlations between the channels to make things interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.multivariate_normal(mean=[0, 0, 0], cov=[[1.0, 0.2, 0.0],[0.2, 1.0, 0.1], [0.0, 0.1, 1.0]], size=1000)\n",
    "plt.plot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the plot now has three different-colored traces? Matplotlib assumes that time is the first dimension when you give it an array to plot.\n",
    "\n",
    "The size along each dimension of the array is called its **shape**. You can get the shape (and therefore the dimension of an array) using the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of y is:\", y.shape)\n",
    "# note that len returns the number of elements along the first dimension\n",
    "print(\"The number of time points in y is:\", len(y))\n",
    "print(\"The total size of y is:\", y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For multichannel data, we need two indices or slices to access values in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first time point in the first channel. Note the comma.\n",
    "y[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use `:` to indicate all the values along one dimension. This gives all the values for the first channel\n",
    "plt.plot(y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all the channels at a time point:\n",
    "print(\"y_0 =\", y[0, :])\n",
    "# you can leave out the trailing indices\n",
    "print(\"y_0 =\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q**: Calculate and plot the mean of all three channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input/Output \n",
    "\n",
    "Presumably you'd like to look at more than just randomly generated noise. So how do you get data from a recording into your program?\n",
    "\n",
    "Usually, data are stored long-term on your computer's drive or in the cloud. There are advantage to both approaches, which we'll discuss later. For now, we're going to retrieve some files that I prepared for you to your local machine. Once we do this, we'll see how to load the data from these files.\n",
    "\n",
    "Executing the following cell will run a shell command to retreive some data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s 'https://gracula.psyc.virginia.edu/public/courseware/comp_neurosci_data_022719.tgz' | tar zxv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### I/O for Time Series\n",
    "\n",
    "Unfortunately, there is no agreed-upon standard for storing time series data, so you'll have to do some sleuthing.\n",
    "\n",
    "There are three major kinds of storage formats: text, binary, and custom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text\n",
    "\n",
    "One way of storing numbers is how you would write them (i.e., as **text**). \n",
    "\n",
    "When reading a text file, the main thing you need to know is how the elements are separated. \n",
    "\n",
    "For single-channel data, usually each number goes on its own line. \n",
    "\n",
    "For multi-channel data, there will be multiple numbers per line, typically separated by white space (tabs and/or spaces) or by commas.\n",
    "\n",
    "When you're storing data in text format, you also need to be mindful of the precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reading text files\n",
    "\n",
    "One advantage of storing data as text is that it's human-readable. However, this isn't as much of an advantage as you might think. \n",
    "\n",
    "To see an example of an extracellular recording in text format, switch to the main Juptyer tab in your browser, navigate to the `data/io-examples` folder, and click on one of the files that ends in `.txt`\n",
    "\n",
    "Numpy can easily load single- and multi-channel data from text files using the `loadtxt` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.loadtxt(\"data/io-examples/st11_1_2_A8.txt\")\n",
    "plt.plot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary\n",
    "\n",
    "Storing numbers as text is very inefficient. Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text is read into Python first as a string:\n",
    "s = open(\"data/io-examples/st11_1_2_A8.txt\", \"r\").readline().strip()\n",
    "print(\"the number as text:\", s)\n",
    "print(\"size of the text (in bytes):\", len(s))\n",
    "\n",
    "## to use it as a number, python has to parse the text\n",
    "f = float(s)\n",
    "print(\"the number as a float:\", f)\n",
    "print(\"size of a float (in bytes):\", d.dtype.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reading binary data\n",
    "\n",
    "Not only does text-formatted data take up a lot more space, it also requires additional work for Python to translate into a numerical representation that it can do math on (i.e., floats and ints).\n",
    "\n",
    "This inefficiency becomes a consideration for large datasets. Thus, we often want to store the data on disk in a binary format, i.e., the same format as it would be in memory.\n",
    "\n",
    "A very powerful method for reading and writing binary data is to use a **memory map**. This essentially takes the contents of a binary file and treats it as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # standard library module used to construct paths\n",
    "\n",
    "d = np.memmap(os.path.join(\"data\", \"io-examples\", \"st11_1_2_A8.dat\"), mode=\"r\", dtype='d')\n",
    "plt.plot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structured formats\n",
    "\n",
    "Text and raw binary formats both have shortcomings and tradeoffs.\n",
    "\n",
    "A shortcoming they both have in common is that it can be difficult to store metadata.\n",
    "\n",
    "Without metadata, it may be hard to know how to interpret the contents of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some critical metadata we need for time series include:\n",
    "\n",
    "- sampling rate\n",
    "- dimensions of the array\n",
    "- ordering of the array (i.e., time first or last) and what's in each channel\n",
    "- measurement units\n",
    "\n",
    "**Q**: What other metadata do you think are important for time series data? Write a couple of ideas in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Structured data formats can be text- or binary-based.\n",
    "\n",
    "Some formats are used widely and are well-documented, like [Javascript Object Notation](http://json.org) or [HDF5](https://support.hdfgroup.org/HDF5/). These formats are likely to have well-supported Python packages for I/O.\n",
    "    \n",
    "Other formats are more obscure or proprietary, like Axon Binary Format (ABF) or [Elan](http://elan.lyon.inserm.fr/). It may be difficult to find Python support to read these files, though the situation is improving thanks to projects like [Neo IO](https://neo.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Point processes\n",
    "\n",
    "Typically represented as an ordered sequence of times in some interval from 0 to $T$:\n",
    "\n",
    "$$\\{0 \\leq t_0 < t_1 < \\ldots < t_N \\leq T\\}$$\n",
    "\n",
    "In contrast to time series, there is not a fixed relationship between the number of events and the duration of the analysis interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point process data in Python\n",
    "\n",
    "Point processes are also typically stored in `numpy` arrays, but the elements of the array are event times, not measurements.\n",
    "\n",
    "Because point processes vary in the number of events, multi-channel point-processes are represented by **lists of arrays**, not by 2D arrays.\n",
    "\n",
    "Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import pprox\n",
    "resp = pprox.load(\"data\", \"starling\", \"pprox\", \"st11_1_2_1\")\n",
    "resp_A8 = pprox.select_stimulus(resp, \"A8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `resp_A8` points to a Python **list**. Lists are like arrays, but they can store heterogeneous data types. The syntax for accessing elements and slices is the same.\n",
    "\n",
    "**Q:** Using what you know from previous exercises, complete the following code cell to print out some information about the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of trials is:\", ???)\n",
    "print(\"The number of events in trial 0 is:\", ???)\n",
    "print(\"The time of the first event in trial 2 is:\", ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O for Point Processes\n",
    "\n",
    "Just as there is no agreed-upon standard for storing time series data, there is also no standard format for point-process data.\n",
    "\n",
    "Because point process data tend to be smaller than time series, text formats are more common than binary.\n",
    "\n",
    "A very simple text format is to put each trial (or channel) on a separate line and separate the events on each line with a space. Take a look at `data/io-examples/st_11_2_1_A8.txt` for an example.\n",
    "\n",
    "The [PySpike](http://mariomulansky.github.io/PySpike/) library has a function for loading data from such files, but we're going to write our own so that we can learn a bit about basic I/O in Python and looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list where we will store our trials\n",
    "trials = []\n",
    "# open the file for reading\n",
    "fp = open(os.path.join(\"data\", \"io-examples\", \"st11_1_2_1_A8.txt\"), mode=\"r\")\n",
    "# loop through the lines of the file with a for statement\n",
    "for line in fp:\n",
    "    # read the line into an array\n",
    "    arr = np.fromstring(line, sep=\" \")\n",
    "    # append the array to our list\n",
    "    trials.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you learned how to program in Java or C or another low-level programming language, take a moment to appreciate how simple this task is in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Checking our work\n",
    "\n",
    "**Q:** The `trials` list we created in the last code cell should be the same as `resp_A8`. In the cell below, complete **three assert** statements to check that this is true. I've provided you with one to get started. If you complete your task correctly, the cell will not emit any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(trials) == len(resp_A8), \"The number of trials is not the same\"\n",
    "assert True == False, \"The total number of events is not the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Point process metadata\n",
    "\n",
    "As with time series data, it's important to keep track of metadata. Here are some important metadata that need to be associated with point process files:\n",
    "\n",
    "- type of event (e.g., spike, behavioral action, stimulus start/stop)\n",
    "- number of channels\n",
    "- unit scaling (e.g., milliseconds or seconds?)\n",
    "- start time\n",
    "- other experimental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structured Records\n",
    "\n",
    "In both point process and time series data, the elements of the arrays have been **homogeneous** (i.e., all the same type). What if that's not the case?\n",
    "\n",
    "The third (and final) kind of data we'll consider today consists of **records**. Each record in turn comprises **fields**, which may have different types.\n",
    "\n",
    "This kind of data is also called **tabular data**. If you're coming from the R world, you might think of this kind of data as a `data.frame`.\n",
    "\n",
    "It's common to encounter structured records when you have independent observations; for example, from different neurons or animals or populations. The fields in each record might include:\n",
    "\n",
    "- a unique identifier for the observation\n",
    "- group identifiers (e.g., cell, animal, population)\n",
    "- independent variables (e.g., sex, treatment, age)\n",
    "- dependent variable(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structured records in Python\n",
    "\n",
    "Python used to lag pretty badly behind R for handling this kind of data, but we now have [pandas](http://pandas.pydata.org/), which is beginning to approach `numpy` in popularity and maturity.\n",
    "\n",
    "As with numpy, there is a convention for importing pandas: \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "For a detailed introduction to pandas, take a look at [Chapter 3](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html) of the Python Data Science Handbook.\n",
    "\n",
    "There are two main concepts to understand in using pandas: `Series` and `DataFrames`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `Series` is essentially a column of a table. Like a numpy array, all the elements of a series are the same type. Unlike a numpy array, the indices of a `Series` do not have to be sequential integers, but can be any label you like.\n",
    "\n",
    "For example, here's a `Series` that might represent the ages of several subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ages = pd.Series([391, 442, 183], index=['st11', 'st22', 'st231'])\n",
    "ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the elements of a `Series` using the standard Python bracket syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages['st11']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `DataFrame` is a collection of `Series`, i.e. a table of columns. Here's how we might represent the ages and sexes of a set of subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = pd.Series(['M', 'F', 'M'], index=['st11', 'st22', 'st231'])\n",
    "subjects = pd.DataFrame({'age': ages, 'sex': sex})\n",
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we now have a table, which Jupyter renders nicely for us with the row and column indices are indicated in bold.\n",
    "\n",
    "The bracket syntax for `DataFrames` accesses **columns**. It's important to remember that this is different from numpy arrays, where a single index gives you a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To index by row and column, you have to use the `loc` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects.loc['st11', 'age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, you can use `iloc` and the numerical indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O for Structured Records\n",
    "\n",
    "Structured record data is usually stored on disk in text-based formats. This is because human readability is often quite important. There are two very common formats:\n",
    "\n",
    "- In _comma-separated-value_ files, each record is on a separate line, and fields are separated by commas.\n",
    "- In _whitespace-delimited-value_ files, each record is on a separate line, and fields are separated by white space (tabs or spaces)\n",
    "\n",
    "In both kinds of files, it's common that the first line of the file is a header giving the name for each column.\n",
    "\n",
    "Take a look at `data/stimuli/motifs.csv` for an example of a comma-delimited file.\n",
    "\n",
    "One really good reason to use pandas is that it provides some nice I/O functions for these kinds of files. It's trivial to load tabular data into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs = pd.read_csv(os.path.join(\"data\", \"starling\", \"stimuli\", \"motifs.csv\"))\n",
    "motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can tell pandas that certain columns should be used as indices.\n",
    "\n",
    "This allows you to select a subset of rows using the `loc` syntax. For example, to see all the rows where `song` is equal to `A8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs = motifs.set_index(['song'])\n",
    "motifs.loc['A8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Write code in the cell below to compute the following:\n",
    "\n",
    "- the number of different songs\n",
    "- the number of motifs for each song\n",
    "- the average motif duration in each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
