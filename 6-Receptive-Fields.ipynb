{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson 6: Receptive Fields\n",
    "\n",
    "In Lesson 5, we discussed linear time-invariant systems, which can serve as a simple model of how the rate of a neuron can depend on a time-varying stimulus.\n",
    "\n",
    "This week, we'll look at some empirical findings about how neurons in the visual system respond to simple stimuli, and then consider how to represent these responses using an LTI model.\n",
    "\n",
    "The assignment for this week is on Collab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "# set some style options\n",
    "mpl.rcParams['image.origin'] = 'lower'\n",
    "mpl.rcParams['image.aspect'] = 'auto'\n",
    "mpl.rcParams['image.cmap'] = 'jet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Let's look at an RF with one spatial and one temporal dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filts = np.load('data/filters.npz')\n",
    "k1 = filts['wb2'] * 25\n",
    "plt.imshow(k1, aspect='equal')\n",
    "plt.xlabel(\"tau (ms)\")\n",
    "plt.ylabel(\"x (px)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to describe what kinds of visual stimuli would best excite a neuron with this RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to simulate the response to some Gaussian white noise. This is equivalent to presenting a field of vertical or horizontal bars that are various shades of gray. I'm only going to show the first 1000 samples so you have a better idea of what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, nt = k1.shape\n",
    "nsamples = nt * 1000\n",
    "np.random.seed(1)\n",
    "stim_raw = np.random.randn(nx, nsamples)\n",
    "stim = stim_raw * 0.1\n",
    "stim[:,:500] = 0\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 4))\n",
    "axes[0].imshow(stim)\n",
    "axes[1].plot(stim[0,:])\n",
    "axes[1].set_xlim(0, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's do the convolution. Remember, to implement in Python we need to convolve each spatial position and then sum them up:\n",
    "\n",
    "\\begin{align}\n",
    "r_{i}(t) & = \\sum_k h(x_i, \\tau_k) s(x_i, t - \\tau_k) \\\\\n",
    "r(t) & = \\sum_i r_{i}(t)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve each pixel (row) separately\n",
    "convrows = [np.convolve(k1[i], stim[i]) for i in range(nx)]\n",
    "# sum across rows\n",
    "conv = np.row_stack(convrows).sum(0)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 4))\n",
    "axes[0].imshow(stim)\n",
    "axes[1].plot(conv[:nsamples])\n",
    "axes[1].set_xlim(0, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result of the convolution to the stimulus and the RF. Is it what you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next comes the \"N\" phase of the LNP model: we have to convert the convolution to an estimated rate by passing it through a nonlinear function. We'll use `exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0 = 1\n",
    "r_est = np.exp(conv[:nsamples] + np.log(r_0))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 4))\n",
    "axes.plot(r_est)\n",
    "axes.set_xlim(0, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And then generate spikes using our standard Bernoulli approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmb = r_est * 0.001\n",
    "n_trials = 10\n",
    "spikes = []\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 4))\n",
    "for i in range(n_trials):\n",
    "    runif = np.random.uniform(size=nsamples)\n",
    "    spk = (lmb > runif).nonzero()[0]\n",
    "    axes.vlines(spk, i - 0.4, i + 0.4)\n",
    "    spikes.append(spk)\n",
    "axes.set_xlim(0, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating RFs\n",
    "\n",
    "As Chapter 2 in Dayan and Abbott discusses in more detail, receptive fields can predict many of the tuning properties of visual neurons. This suggests two key conclusions:\n",
    "\n",
    "1. Visual neurons (up to a point) are pretty linear.\n",
    "2. Linear models have a lot of explanatory power.\n",
    "\n",
    "As a consequence, we often want to try to estimate the linear RF of sensory neurons. Even if the neuron isn't totally linear, it's a good place to start. But how do we infer the kernel in a system where we can't stimulate with a delta function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Let's see how we would do spike-triggered averaging using the response we just simulated. First, let's look at the stimulus that precedes one of the spikes in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spikes[0][5]\n",
    "sts = stim[:,t-nt:t]\n",
    "plt.imshow(sts, aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like much, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what happens if we average over many, many spikes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# using an accumulator pattern to count spikes \n",
    "n_spikes = 0\n",
    "# each stimulus will be added into this empty array\n",
    "sta = np.zeros_like(k1)\n",
    "for trial in spikes:\n",
    "    for t in trial:\n",
    "        # exclude spikes that are too close to the start\n",
    "        if t - nt < 0: continue\n",
    "        n_spikes += 1\n",
    "        sta += stim[:,t-nt:t]\n",
    "# average by dividing the sum by N\n",
    "sta /= n_spikes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(9, 4))\n",
    "axes[0].imshow(sta, aspect=\"equal\")\n",
    "axes[0].set_title(\"STA (n=%d)\" % n_spikes)\n",
    "axes[1].imshow(k1, aspect='equal')\n",
    "axes[1].set_title(\"RF (true)\")\n",
    "axes[0].set_xlabel(\"tau (ms)\")\n",
    "axes[0].set_ylabel(\"x (px)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! The average stimulus preceding the spikes produced by this neuron looks like a time-flipped version of the receptive field.\n",
    "\n",
    "If the stimulus is random and uncorrelated, given enough spikes the reverse correlation will always recover the receptive field.\n",
    "\n",
    "What happens if we reduce the average firing rate (for example by setting the baseline $r_0$ to less than 1 Hz)? What happens if we \"record\" for more or less time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stimulus correlations\n",
    "\n",
    "Real-world stimuli rarely look like white noise. In the visual domain, stimuli are dominated by low frequencies, because objects tend to be fairly similar intensity surrounded by sharp edges.\n",
    "\n",
    "In other words, pixels tend to be correlated with neighboring pixels and tend to change slowly over time. What happens if we try to use a stimulus that has some correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "stim_filt = ndimage.gaussian_filter(stim_raw, sigma=(3, 50))\n",
    "stim_filt[:,:500] = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 4))\n",
    "axes.imshow(stim_filt)\n",
    "axes.set_xlim(0, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code cell combines all the steps in the LNP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(12, 6))\n",
    "# convolve each pixel (row) separately\n",
    "convrows = [np.convolve(k1[i], stim_filt[i]) for i in range(nx)]\n",
    "# sum across rows\n",
    "conv = np.row_stack(convrows).sum(0)\n",
    "\n",
    "axes[0].imshow(stim_filt)\n",
    "axes[1].plot(conv[:nsamples])\n",
    "axes[1].set_xlim(0, nsamples)\n",
    "\n",
    "r_0 = 1.0\n",
    "r_est = np.exp(conv[:nsamples] + np.log(r_0))\n",
    "lmb = r_est * 0.001\n",
    "n_trials = 10\n",
    "spikes = []\n",
    "\n",
    "axes[2].plot(r_est)\n",
    "for i in range(n_trials):\n",
    "    runif = np.random.uniform(size=nsamples)\n",
    "    spk = (lmb > runif).nonzero()[0]\n",
    "    axes[3].vlines(spk, i - 0.4, i + 0.4)\n",
    "    spikes.append(spk)\n",
    "axes[3].set_xlim(0, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens when we try to estimate the kernel from this response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# now we'll do this for every spike\n",
    "n_spikes = 0\n",
    "sta = np.zeros_like(k1)\n",
    "for trial in spikes:\n",
    "    for t in trial:\n",
    "        # exclude spikes that are too close to the start\n",
    "        if t - nt < 0: continue\n",
    "        n_spikes += 1\n",
    "        sta += stim_filt[:,t-nt:t]\n",
    "# average by dividing the sum by N\n",
    "sta /= n_spikes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(9, 4))\n",
    "axes[0].imshow(sta, aspect=\"equal\")\n",
    "axes[0].set_title(\"STA (n=%d)\" % n_spikes)\n",
    "axes[1].imshow(k1, aspect='equal')\n",
    "axes[1].set_title(\"RF (true)\")\n",
    "axes[0].set_xlabel(\"tau (ms)\")\n",
    "axes[0].set_ylabel(\"x (px)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What happened?? The STA looks nothing like the kernel, even though we averaged data from roughly the same number of spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Very important conclusion**: The spike-triggered average only recovers the kernel of an LTI system if there are no correlations in the stimulus. \n",
    "\n",
    "To see why this is, think about how the average would be affected if the ensemble of black points is not spherical:\n",
    "\n",
    "<img src=\"images/l8_sta_ensemble.png\" alt=\"spike-triggered average\" style=\"width: 450px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel estimation as linear regression\n",
    "\n",
    "The solution to the foregoing problem is to correct for the correlations in the stimulus. How do we do this? It took the field a suprisingly long time to figure this out.\n",
    "\n",
    "It can be helpful to recast the problem as one of linear regression. Remember the expansion of convolution:\n",
    "\n",
    "$$r(t) = h_1 s(t) + h_2 s(t-\\Delta) + h_3 s(t-2\\Delta) + \\cdots + h_N(t-N\\Delta)$$ \n",
    "\n",
    "Here we've represented the kernel with subscripts starting with 1 but kept the functional notation for the stimulus.\n",
    "\n",
    "We can rewrite this sum as a dot product,\n",
    "\n",
    "$$r(t) = \\mathbf{s}(t) \\cdot \\mathbf{h}$$\n",
    "\n",
    "where $\\mathbf{s}(t)$ refers to the **time-lagged** stimulus. That is $\\mathbf{s}(t) = \\{s(t), s(t - \\Delta), \\ldots, s(t - N\\Delta)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Let's look at a concrete example in numpy using a really short stimulus and kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the kernel\n",
    "h = np.asarray([0, 1, -0.5])\n",
    "# the full stimulus\n",
    "stim = np.random.randn(20)\n",
    "plt.plot(stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the response at $t = 2$. $\\mathbf{h}$ is time-invariant, so we leave it as is. $\\mathbf{s}(2)$ is `[stim[0], stim[1], stim[2]]` but **in reverse**. In slice notation, this is `stim[2::-1]` (i.e., start at index 2 and go to the beginning in steps of -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r_2 =\", np.dot(h, stim[2::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectorizing the model\n",
    "\n",
    "Instead of writing the response and stimulus as a function of time, let's think of them as a series of observations made at discrete intervals. Thus,\n",
    "\n",
    "$$r(t) = \\mathbf{s}(t) \\cdot \\mathbf{h}$$\n",
    "\n",
    "becomes\n",
    "\n",
    "$$r_i = \\mathbf{s}_i \\cdot \\mathbf{h}$$\n",
    "\n",
    "where $r_i$ is the rate at time $t_i$ and $\\mathbf{s}_i$ is the stimulus from $t_{i-N}$ to $t_i$.\n",
    "\n",
    "We can further simplify our notation by stacking all the observations in a vector $\\mathbf{r}$:\n",
    "\n",
    "$$\\mathbf{r} = \\mathbf{S} \\mathbf{h}$$\n",
    "\n",
    "The matrix $\\mathbf{S}$ has as many rows as there are time points in the observation vector $\\mathbf{r}$ and as many columns as there are time points in the kernel $\\mathbf{h}$. In each row, it contains the stimulus at the current time and at a set of previous lags. \n",
    "\n",
    "Let's see what this would look like for our toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(\"stimulus:\")\n",
    "display(stim)\n",
    "ntau = len(h)\n",
    "nt = len(stim)\n",
    "S = np.zeros((nt - ntau, ntau))\n",
    "for i in range(ntau, nt):\n",
    "    S[i-ntau] = stim[i-ntau:i][::-1]\n",
    "print(\"stimulus matrix:\")\n",
    "display(S)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a close look at the stimulus matrix. Notice how the values shift over by one in each row?\n",
    "\n",
    "This kind of matrix is called a [Toeplitz matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix), and it's the linear algebra equivalent of convolution.\n",
    "\n",
    "There's a function in `scipy` that can generate this matrix and even do some padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import toeplitz\n",
    "S = toeplitz(stim, np.asarray([stim[0], 0, 0]))\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With our Toeplitz matrix, convolution is as simple as matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the slow way\n",
    "r1 = np.convolve(stim, h)[:nt]\n",
    "# and the fast way\n",
    "r2 = np.dot(S, h)\n",
    "# should give the same result....\n",
    "assert np.all(r1 == r2)\n",
    "display(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is r_2 the same as what we manually calculated above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear regression with time-varying stimuli\n",
    "\n",
    "What does this have to do with linear regression? The advantage of this notation is that it's easy to see how our data consist of a series of observations made for different values of $\\mathbf{s}$. Some of the variance in these observations is explained by variation in the stimulus; the rest is random noise. We represent this noise as an additional variable that's added at each time point. \n",
    "\n",
    "$$r_i = \\mathbf{s}_i \\cdot \\mathbf{h} + \\varepsilon_i$$\n",
    "\n",
    "Or equivalently,\n",
    "\n",
    "$$\\mathbf{r} = \\mathbf{S} \\mathbf{h} + \\mathbf{\\varepsilon}$$\n",
    "\n",
    "Given this model, our goal is to estimate $\\mathbf{h}$. That is, to find values that maximize the amount of variance explained by the stimulus and minimize the amount of random error ($\\mathbf{\\varepsilon}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we assume that the noise is normally distributed and independent, then what we need to do is minimize the sum of the squares of the error. This is called **ordinary least squares** (OLS).\n",
    "\n",
    "Hopefully, you've seen that our model is a pretty vanilla linear model. The OLS solution is well-known:\n",
    "\n",
    "$$\\hat{h} = (\\mathbf{S}^T \\mathbf{S})^{-1} \\mathbf{S}^T \\mathbf{r} = \\left(\\sum_i \\mathbf{s}_i \\cdot \\mathbf{s}_i \\right)^{-1} \\left(\\sum_i \\mathbf{s}_i r_i \\right)$$\n",
    "\n",
    "- The first term is the autocovariance matrix for the independent variables. \n",
    "- The second term is the covariance between the dependent and independent variables. \n",
    "- Here, these correspond to the **autocorrelation** and the **cross-correlation**\n",
    "\n",
    "If the stimulus is white noise, the correlation between any two instants in time is zero, so $\\mathbf{S}^T\\mathbf{S} = \\sigma^2\\mathbf{I}$ ($\\mathbf{I}$ is the identity matrix and $\\sigma^2$ is the variance of the stimulus).\n",
    "\n",
    "The $^{-1}$ operator stands for **matrix inversion**. This is what \"undoes\" the effects of the correlations in the stimulus. Matrix inversion is computationally expensive and numerically hairy. There are some tricks for dealing with the latter, but for now let's look at a quick example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Let's use a nice alpha function as our kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "dt = 1.0\n",
    "kt = np.arange(0, 75, dt)\n",
    "k = kt / tau**2 * np.exp(-kt / tau)\n",
    "nkt = len(k)\n",
    "plt.plot(kt, k);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And let's consider two stimuli: one that's uncorrelated and one that is correlated (i.e. smoothed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nt = 10000\n",
    "t = np.arange(0, nt, 1.0)\n",
    "stim_u = np.random.randn(nt)\n",
    "stim_c = ndimage.gaussian_filter(stim_u, 5)\n",
    "plt.plot(t, stim_u, t, stim_c)\n",
    "plt.xlim(0, 200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll generate our Toeplitz matrices for both stimuli and use them to simulate a response. Note that we're adding a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_u = toeplitz(stim_u[nkt:], stim_u[nkt-1::-1])\n",
    "S_c = toeplitz(stim_c[nkt:], stim_c[nkt-1::-1])\n",
    "\n",
    "nrt = nt - nkt\n",
    "eps = np.random.randn(nrt) * 0.05\n",
    "r_u = np.dot(S_u, k) + eps\n",
    "r_c = np.dot(S_c, k) + eps\n",
    "plt.plot(r_u)\n",
    "plt.plot(r_c)\n",
    "plt.xlim(0, 200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The STA is just the inverse of the convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STA_u = np.dot(S_u.T, r_u)\n",
    "STA_c = np.dot(S_c.T, r_c)\n",
    "plt.plot(kt, STA_u / nt, kt, STA_c / nt, kt, k, 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the STA for the uncorrelated stimulus is pretty good, but the STA for the correlated one is badly distorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's find the OLS estimate by dividing out the stimulus autocorrelations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "# this works because the covariance matrix is relatively easy to invert\n",
    "cov_u = np.dot(S_u.T, S_u)\n",
    "ols_u = np.dot(linalg.inv(cov_u), STA_u)\n",
    "\n",
    "# we have to do a little regularization to avoid numerical issues\n",
    "cov_c = np.dot(S_c.T, S_c) + 10 * np.eye(nkt)\n",
    "ols_c = np.dot(linalg.inv(cov_c), STA_c)\n",
    "\n",
    "plt.plot(kt, ols_u, kt, ols_c, kt, k, 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the estimate from the correlated stimulus is much closer to the true kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- The receptive field of a neuron can be approximated as the linear kernel of a linear-nonlinear-Poisson model\n",
    "- If the stimulus distribution is spherical and Gaussian, the RF/kernel can be estimated using spike-triggered averaging\n",
    "- If the stimulus distribution is not spherical, the RF/kernel can be estimated using linear correlation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
