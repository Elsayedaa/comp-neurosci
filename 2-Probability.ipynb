{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2 Probability, Distributions, and Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2A More on Python\n",
    "\n",
    "If you're new to programming in general, you should be working your way through all the chapters of the [Python for Data Science](https://www.datacamp.com/courses/intro-to-python-for-data-science) DataCamp tutorial.\n",
    "\n",
    "If you're a more accomplished programmer but new(ish) to Python, you can get a more detailed primer on the same material by going through chapters 1-4 of the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/).\n",
    "\n",
    "Assuming you've completed these activities, the sections below will be a review of some key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objects and packages\n",
    "\n",
    "Python is firmly grounded in the tradition of *object-oriented programming* (OOP). In this approach, the logic for dealing with different kinds of data is *encapsulated* or *attached* to the data. A data object can\n",
    "\n",
    "- have *attributes* that tell us about that object instance\n",
    "- have *methods* that are functions an object can perform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is an instance?\n",
    "\n",
    "Every value in Python is an *instance* of some type. Each time you create a new value, the interpreter is *instantiating* an object of a defined type, allocating memory and setting initial values.\n",
    "\n",
    "Thus, any variable we define is simply pointing the variable's name (in the current namespace) to a chunk of memory containing the instance.\n",
    "\n",
    "Python keeps track of all object intances and cleans them when they are no longer needed.\n",
    "\n",
    "You can determine the type of a value or variable using the `type` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = 42\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The attributes and methods of an object are accessed by using the `.` operator. You can *inspect* the attributes and methods of any object in the notebook by typing `tab` after entering the variable name and `.`, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore x:\n",
    "# (place the cursor after x. and press the tab key)\n",
    "x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try with a different type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y = 'The answer to the ultimate question.'\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the string type:\n",
    "# (press tab after y.)\n",
    "y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can access documentation for functions and methods by using `Shift-Tab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place the cursor between the parentheses and type `Shift-Tab`\n",
    "y.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a package?\n",
    "\n",
    "Programming is all about abstraction, and code is meant to be reused. Python has tons of useful packages that extend its basic functionality. This means you don't have to (and shouldn't) reinvent basic algorithms. \n",
    "\n",
    "We'll talk more about how to find and evaluate packages later.\n",
    "\n",
    "You can access external packages (or modules) by **importing** them into your workspace. When you import a package, you should give it a short name that you'll use to access the functions, etc, in the package.\n",
    "\n",
    "You'll see (and start to use) lots of statements like the following:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "After executing this statement, you can access the contents of the module as if they were attributes and methods of an object called `np`, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Arrays\n",
    "\n",
    "The `numpy` package provides Python with an important data structure, the **array**. Arrays are so important to scientific programming that you will almost inevitably import `numpy` at the beginning of every script and notebook you use.\n",
    "\n",
    "As we discussed previously, **scalar** data types like `int` and `float` can only represent a single number. But time series and point processes are inherently ordered collections of data, and arrays give us the ability to store and manipulate these large aggregates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Array terminology\n",
    "\n",
    "- An **element** of an array is one item. It occupies a specific \"slot\" in the sequence.\n",
    "- The **length** or **size** of an array is the number of elements.\n",
    "- The **index** of an element is the numerical position of the element in the array. Python uses *zero-based indexing*, which means that the first element has the index `0`.\n",
    "- The **data type** of the array is the type of the elements in the array. In arrays, all the elements have the same type. In `numpy`, the data type is stored in the `dtype` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Array operations\n",
    "\n",
    "- **indexing** allows you to access specific elements of an array\n",
    "- **slicing** allows you to access specific subsets of an array\n",
    "- **iteration** allows you to process each element of the array in sequence\n",
    "\n",
    "In numpy, both indexing uses square brackets (`[` and `]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's initialize the random seed so that we all get the same answers\n",
    "np.random.seed(1)\n",
    "# create an array with 100 random numbers\n",
    "my_data = np.random.randn(100)\n",
    "# use indexing to get the first element:\n",
    "my_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Slicing also uses square brackets, but instead of a single index, you use two indices separated by `:` to specify a range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will retrieve the first 10 elements of my_array\n",
    "my_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how indexing returns a scalar, but slicing returns a new array. Also notice that the last index is *exclusive*; that is, element 10 is NOT returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Some simple problems\n",
    "\n",
    "Write an expression for the 88th value of `my_array` in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an expression for the last 10 elements of `my_array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2B Basics of Probability\n",
    "\n",
    "Let's consider two events, $A$ and $B$.\n",
    "\n",
    "**The probability of $A$**\n",
    "\n",
    "$$P(A) \\in [0, 1]$$\n",
    "\n",
    "**The probability of not $A$**\n",
    "\n",
    "$$1 - P(A)$$\n",
    "\n",
    "**The probability of $A$ *or* $B$**\n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$ \n",
    "\n",
    "If $A$ and $B$ are mutually exclusive, \n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B)$$\n",
    "\n",
    "**The probability of $A$ and $B$. This is also called the joint probability**\n",
    "\n",
    "$$P(A,B) = P(A|B) P(B) = P(B|A) P(A)$$\n",
    "\n",
    "If $A$ and $B$ are independent,\n",
    "\n",
    "$$P(A,B) = P(A) P(B)$$\n",
    "\n",
    "**The probability of $A$ given $B$. This is called the conditional probability**\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple examples\n",
    "\n",
    "Using the table above, let's try and figure out the probabilities of the following:\n",
    "\n",
    "1. Rolling a 5 on a 6-sided die.\n",
    "2. Not rolling a 3 on a 6-sided die.\n",
    "3. Rolling a 4 or a 5 on a 6-sided die.\n",
    "4. Rolling less than 4 or an even number on a 6-sided die.\n",
    "5. Rolling two 3's in a row on a 6-sided die.\n",
    "\n",
    "Enter your answers as text in the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Harder question\n",
    "\n",
    "#### The case of Tim Tebow\n",
    "\n",
    "What is the probability of becoming a Major League Baseball (MLB) player if you hit a home run (HR) in your first at-bat in the minor leagues? We have the following important information:\n",
    "\n",
    "a) 5% of future MLB players hit a home run in their first minor league at-bat.\n",
    "\n",
    "b) 1% of minor league players make it to MLB.\n",
    "\n",
    "c) Only 0.1% of players hit a homerun in their first minor league at-bat.\n",
    "\n",
    "Hint: You are trying to solve $P(MLB \\mid HR)$.\n",
    "\n",
    "Bonus: What percentage of players who don't make it to MLB hit a home run in the first at bat?\n",
    "\n",
    "Write code to answer the question in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probability Distributions\n",
    "\n",
    "At the core of probability theory are the mathematical functions determining the probability of the potential outcomes of an experiment. \n",
    "\n",
    "In statistics, these distributions represent the models that attempt to describe the observed data. The equations take in parameters that determine the shape of the probability distributions.\n",
    "\n",
    "Let's explore some continuous and discrete probability distributions relevant to quantifying data and models!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "import ipywidgets as widgets      # interactive widgets\n",
    "\n",
    "# import some distributions\n",
    "from tools.dists import uniform, normal, beta, gamma, invgamma, exp, poisson, laplace, students_t, noncentral_t, halfcauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell defines a function that will let us plot probability distributions easily\n",
    "\n",
    "def plot_pdf(dist, support=[-5, 5], npoints=100):\n",
    "    \"\"\"Plot a probability density function over a support interval.\n",
    "    \n",
    "    dist - a scipy distribution function\n",
    "    support - the range of values over which to evaluate the PDF\n",
    "    npoints - the number of points within the support to evaluate\n",
    "    \"\"\"\n",
    "    x = np.linspace(support[0], support[1], npoints)\n",
    "    pdf = dist.pdf(x)\n",
    "    plt.plot(x, pdf, lw=3)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Uniform\n",
    "\n",
    "A continuous probability distribution assigning equal probability over a range.\n",
    "\n",
    "What happens when we change the range?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the PDF\n",
    "plot_pdf(uniform(lower=-2, upper=2))\n",
    "plot_pdf(uniform(lower=-1, upper=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Normal\n",
    "\n",
    "Need I say more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(normal(mean=0, std=1))\n",
    "plot_pdf(normal(mean=1, std=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Beta\n",
    "\n",
    "Only has support between 0 and 1. Useful to help determine the probability of a probability.\n",
    "\n",
    "We'll spend some time with Beta distributions in subsequent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(beta(alpha=0.5, beta=0.5), support=[0,1])\n",
    "plot_pdf(beta(alpha=2, beta=5), support=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### And more...\n",
    "\n",
    "Take some time now to explore:\n",
    "\n",
    "- Gamma\n",
    "- Inverse Gamma\n",
    "- Exponential\n",
    "- Student's t\n",
    "- Half Cauchy\n",
    "- Poisson\n",
    "\n",
    "**Assignment**: choose two distributions. Try to replicate the illustrative plots on their respective pages on Wikipedia. Insert code cells below to generate the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Observation and Inference\n",
    "\n",
    "Why is probability theory important to computational neuroscience? \n",
    "\n",
    "Fundamentally, the problem we face is that we can't directly determine any physical quantity. We can only take a measurement of it, and that measurement will have errors. Thus, each time we make a measurement, we are going to get a value that comes from a **distribution**.\n",
    "\n",
    "Even more troubling, we rarely are able to directly measure the actual quantities we care about. Think back to the last exercise (or run it, if you don't remember!). It was clear that a limited set of sounds were activating the neuron (i.e., generating synaptic excitation), but we didn't have a direct measurement of how strong that excitation was. We could only observe that the neuron produced action potentials at higher rates during certain intervals.\n",
    "\n",
    "The process of using observations to gain information about unobservable quantities is called **inference** or **estimation**, and probability theory gives us the tools we need to make this connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling and Measurement\n",
    "\n",
    "Let's come up with a **observational model** that formalizes what we think is going on when we make a measurement.\n",
    "\n",
    "Observational models are fundamental to all statistical analyses and will be a part of many of the more complex models we develop.\n",
    "\n",
    "Let's say that we have a bar of length $\\mu$. We can measure the bar's length as many times as we like, and each time we do so, we'll get a value:\n",
    "\n",
    "$$y_i = \\mu + \\varepsilon_i$$\n",
    "\n",
    "Notice the subscript $i$. This is a numerical **index** for the measurement. Let's stick with the Python convention and have $y_0$ indicate the value of the first measurement.\n",
    "\n",
    "This model simply says that any given measurement $y_i$ will be the true length of the bar plus some (hopefully small) error $\\varepsilon_i$. That is, we're assuming **additive error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The normal error model\n",
    "\n",
    "There are many potential sources of error in our measurement.\n",
    "\n",
    "The bar might be fluctuating slightly in length due to changes in temperature. This is an example of an **intrinsic error**.\n",
    "\n",
    "The instrument making the measurement might have limited precision due to how it's constructed or because of electrical interference. This is an example of **extrinsic error**.\n",
    "\n",
    "Thanks to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), a reasonable assumption is that the sum of all these sources of error will have a normal (or Gaussian) distribution.\n",
    "\n",
    "We can formalize this assumption by saying that $\\varepsilon_i$ is **drawn** or **sampled** from a normal distribution. This relationship is often signified with $\\sim$.\n",
    "\n",
    "$$\\varepsilon_i \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "$N$ represents the normal distribution, and as we saw above, this distribution has two parameters: mean and variance.\n",
    "\n",
    "Why is the mean for the error distribution zero? What does the variance correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulating measurements\n",
    "\n",
    "Python can generate numbers from a distribution. The numbers are usually not truly random, because computers are (generally) deterministic, but the values will occur with the probability specified by the underlying PDF. Let's generate 10 measurements of our bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import data\n",
    "# true length of bar:\n",
    "mu = 12.1\n",
    "# number of measurements\n",
    "N = 10\n",
    "# errors\n",
    "epsilon = normal(mean=0.0, std=data.e2_std).rvs(N)\n",
    "# measurements\n",
    "y = mu + epsilon\n",
    "\n",
    "# inspect the variables\n",
    "print(\"errors:\", epsilon)\n",
    "print(\"measurements:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside: numpy arrays support **broadcasting**, which is what allows us to simply add a scalar `mu` to an array `epsilon` and get a new array in which `mu` has been added to every element of `epsilon`. If you're used to lower-level langauges like C or Java, make sure you take advantage of this feature, as it's MUCH faster than iterating through the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Histograms\n",
    "\n",
    "It's useful to look at the raw data, but distributions are usually better visualized as **histograms**. A histogram is a plot that divides a range into a set of intervals or **bins**, and then counts the number of values in each bin.\n",
    "\n",
    "Matplotlib has a histogram function, so no need to reinvent the wheel here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, bins, _ = plt.hist(y, range=(10, 14), bins=10, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! All the observations fall in two bins. Edit the code cell above to change the `range` and/or `bins` parameters so that the plot gives you more useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting the observational model\n",
    "\n",
    "Once you've got a nice histogram, try fitting it to a normal PDF. Copy the plot statement from the code cell above into the cell below, then adjust the mean and std parameters in the `plot_pdf` until you get what looks like a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COPY histogram command in the line below\n",
    "\n",
    "### EDIT this line to adjust the PDF\n",
    "plot_pdf(normal(mean=???, std=???), support=(11, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean` and `std` parameters you settle on are called your `estimate`.\n",
    "\n",
    "It may not be possible to achieve a good fit with only two observations. What effect does this have on your ability to come up with good parameter estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Explore whether increasing the sample size helps. Copy and paste ONLY the relevant lines of code from above and paste them into the cell below. (Normally we avoid copying and pasting like the plague, but this will help you to think about what each statement is doing so that you can choose ONLY the relevant ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paste your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary statistics\n",
    "\n",
    "Obviously, manually fitting a PDF to the histogram is both tedious and error-prone.\n",
    "\n",
    "Because we're using a normal error model, we can estimate the parameters directly by computing the mean and standard deviation.\n",
    "\n",
    "Consult the numpy documentation (see link under the `Help` menu) and find the array methods that will compute these summary statistics, then edit the code cell below so that it prints out the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean is\", y.<insert-method-call-here>)\n",
    "print(\"The standard deviation is\", y.<insert-method-call-here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close were your estimates to the summary statistics? How close are the summary statistics to the values of $\\mu$ and $\\sigma$ that were used to generate the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus activity\n",
    "\n",
    "If you have time, try fitting a normal additive error model to data that were generated using **multiplicative error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = data.mult_error_data(mu, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "comp-neurosci",
   "language": "python",
   "name": "comp-neurosci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
