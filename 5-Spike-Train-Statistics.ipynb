{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 5: Spikes and spike train statistics\n",
    "\n",
    "As we've seen from previous examples, much of the data generated by the brain consists of spikes (i.e., action potentials). We all know that neurons spike when they are excited, but what does this mean quantitatively?\n",
    "\n",
    "In this lesson, we'll dive into a fairly simple but foundational model that attempts to quantify spiking as the function of an underlying **rate**.\n",
    "\n",
    "Our goals are to:\n",
    "\n",
    "- understand homogeneous and inhomogeneous Poisson process models\n",
    "- be able to estimate the latent rate variable in Poisson models\n",
    "\n",
    "Follow along in the notebook during the lecture, and then work on the cells marked **Q** with help from your instructor. Submit the completed notebook to Collab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More point process math\n",
    "\n",
    "Recall that a point process is an (ordered) sequence of event times:\n",
    "\n",
    "$$X = \\{t_0, t_1, \\ldots, t_{N-1}\\}$$\n",
    "\n",
    "We can represent this as a function by making each spike a [Dirac delta function](https://en.wikipedia.org/wiki/Dirac_delta_function):\n",
    "\n",
    "$$\\rho(t) = \\sum_{i=0}^{N-1} \\delta(t - t_i)$$\n",
    "\n",
    "![spike_train](images/l5_spike_train.png \"spike train delta function\")\n",
    "\n",
    "Because the area under each delta function is 1, this allows us to count spikes or calculate any continuous function of a spike train through integration.\n",
    "\n",
    "For example, the **rate** is defined as the number of spikes $N$ that occurred in some interval divided by the duration of the interval, $T$:\n",
    "\n",
    "$$R = \\frac{N}{T} = \\frac{1}{T} \\int_{0}^{T} d\\tau\\; \\rho(\\tau)$$\n",
    "\n",
    "![spike_train_rate](images/l5_rate_integral.png \"spike train rate integral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spiking as a random variable\n",
    "\n",
    "Neural responses are **stochastic**. Even under \"identical\" conditions, spike trains will vary from trial to trial.\n",
    "\n",
    "In other words, the response $\\rho(t)$ is a **random variable**. The probability of observing a particular response is given by a **distribution**, $p(\\rho(t))$.\n",
    "\n",
    "We can also represent the stimulus $\\vec{s}(t)$ as a random variable with a distribution $p(\\vec{s}(t))$, even if it's under experimental control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review of probability\n",
    "\n",
    "Leaving out the notation indicating that both $\\rho$ and $\\vec{s}$ are functions of time, the following distributions are of interest:\n",
    "\n",
    "The joint distribution $p(\\rho, \\vec{s})$ is the probability of $\\rho$ and $\\vec{s}$ occurring together. This is likely to be a very small number because the space of $\\vec{s}$ and $\\rho$ are potentially quite large.\n",
    "\n",
    "Thus, we are often more interested in the conditional probability $p(\\rho|\\vec{s})$, which is the probability of observing $\\rho$ when $\\vec{s}$ is presented.\n",
    "\n",
    "The marginal probability $p(\\rho)$ indicates the probability of observing $\\rho$ irrespective of which stimulus was presented.\n",
    "\n",
    "There are two mathematical identities that allow us to convert between these distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conditional Probability\n",
    "\n",
    "$$p(\\rho,\\vec{s}) = p(\\rho|\\vec{s})p(\\vec{s}) = p(\\vec{s}|\\rho)p(\\rho)$$\n",
    "\n",
    "If $p(\\rho|\\vec{s}) = p(\\rho)$, then $\\rho$ and $\\vec{s}$ are **independent**. Independence means $p(\\rho)$ is the same regardless of what $\\vec{s}$ is. This means that for independent variables, the joint distribution is simply the product of the marginal distributions.\n",
    "\n",
    "$$p(\\rho,\\vec{s}) = p(\\rho)p(\\vec{s})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Marginal Probability\n",
    "\n",
    "We can convert a joint probability distribution to a marginal probability distribution by summing (integrating) the probabilities of one of the variables.\n",
    "\n",
    "$$p(\\rho) = \\int_S d\\vec{s}\\; p(\\rho, \\vec{s}) = \\int_S d\\vec{s}\\; p(\\rho|\\vec{s}) p(\\vec{s})$$\n",
    "\n",
    "The $S$ under the integral means that we are summing up $p(\\rho,\\vec{s})$ over every possible stimulus. Remember that every probability density function has to integrate to 1.0:\n",
    "\n",
    "$$\\int_S d\\vec{s}\\; p(\\vec{s}) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spike-train statistics\n",
    "\n",
    "Let's apply these concepts to spike trains:\n",
    "\n",
    "The probability of of a sequence of $N$ spikes $X = \\{t_0,\\ldots,t_{N-1}\\}$ is the joint probability density of all the individual spikes: \n",
    "\n",
    "$$p(t_0, t_1, \\ldots, t_{N-1})$$\n",
    "\n",
    "If the spikes are independent, then this joint distribution is simply the product of the distributions for each spike:\n",
    "\n",
    "$$p(t_0, \\ldots, t_{N-1}) = \\prod_{i=0}^{N-1}p(t_i)$$\n",
    "\n",
    "When each spike is independent of every other spike, we have a **Poisson process**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Homogeneous Poisson Processes\n",
    "\n",
    "If $t_i$ is independent of all the other spikes, what does it depend on?\n",
    "\n",
    "In the simplest case, $p(t_i)$ is a constant: the probability of observing a spike at any given time is a single number, which corresponds to the **rate** of spiking, $R$.\n",
    "\n",
    "If the rate is constant, the Poisson process is **homogeneous**. In this case, in an interval $(t_i, t_i + \\Delta)$, we would expect to observe $\\lambda = R\\Delta$ events. The distribution of the number of events we actually observe, $n$, is given by the Poisson distribution:\n",
    "\n",
    "$$p(n|\\lambda) = \\frac{\\lambda^n}{n!}\\exp(-\\lambda)$$\n",
    "\n",
    "The parameter $\\lambda$ is often called the **intensity** of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore some properties of the Poisson distribution in Python. Here's a graph of the distribution from wikipedia:\n",
    "\n",
    "![poisson_distro](https://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib notebook\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "# notebook modules\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# import the poisson distro\n",
    "from tools import dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first, let's define our *support*: the values over which we want to evalute p(n):\n",
    "supp = np.arange(0, 20)\n",
    "\n",
    "# next, we *instantiate* the distribution object with our parameter lambda\n",
    "dist = dists.poisson(1.0)\n",
    "\n",
    "# you can get the probability of any value in the distribution with .pmf. Note that we have to use\n",
    "# pmf (probability mass function) rather than pdf.\n",
    "print(\"p(5|lambda=1) =\", dist.pmf(5))\n",
    "\n",
    "# we can also evaluate the distribution over a vector of numbers\n",
    "prob = dist.pmf(supp)\n",
    "\n",
    "# and plot the distribution with plt.plot\n",
    "plt.plot(supp, prob, lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q**: In the cell below, try to evaluate the `prob` distribution for negative or non-integral numbers. Given the definition of the Poisson distribution, why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q** Recall that $\\lambda = R\\Delta$. If $R = 1$ Hz and you steadily reduce $\\Delta$ from 1.0 s to 1.0 microseconds, what is the probability of observing one spike in that interval? Write a *for loop* to evaluate and plot this. Does the result make sense? What is the probability of a spike occurring at some *exact* time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = [1e0, 5e-1, 1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition to plotting the probability distribution, Python can generate random samples (i.e, **draw**) from the Poisson distribution, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: 'lambda' is a reserved symbol in python\n",
    "lam = 1.0\n",
    "dist = dists.poisson(lam)\n",
    "# rvs stands for random value(s)\n",
    "n = dist.rvs(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q** In the cell below, calculate the sample mean and standard deviation for n, then try with a few other values of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The sample mean when lambda=\", lam, \"is\", np.mean(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Based on your exploratory analysis, fill out the following identities:\n",
    "\n",
    "- Mean: $\\mu =$\n",
    "- Standard deviation: $\\sigma =$\n",
    "- Fano factor $\\sigma^2/\\mu =$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Poisson distribution to Poisson process\n",
    "\n",
    "How can we generate a series of spike times from the Poisson distribution? The trick is to divide your response interval up into a set of smaller intervals (or **bins**) such that the probability of observing more than one spike in a single bin is very small, then draw from $p(n|\\lambda)$ for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "T     = 100     # s\n",
    "rate  = 4.0     # Hz\n",
    "Delta = 0.005   # s\n",
    "dist   = dists.poisson(rate * Delta)\n",
    "hom_spikes = dist.rvs(int(T / Delta))\n",
    "bins   = np.arange(0, T, Delta)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 3))\n",
    "axes.plot(bins, hom_spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The plot of the `spikes` array should only show `0` and `1` values. If it doesn't, try adjusting the `Delta` variable in the code cell above. What direction does it need to change to fix the problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spike arrays and spike times\n",
    "\n",
    "You can think of the `spikes` array as a sort of time series representation of the point process. \n",
    "\n",
    "To get the actual spike times, we need to find the bins where there is a spike and then look up the times in the `bins` array. This allows us to generate a *raster plot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_spike_i = np.nonzero(hom_spikes)[0]\n",
    "hom_spike_t = bins[hom_spike_i]\n",
    "# here's one way to plot a raster of spike times\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 3))\n",
    "axes.plot(hom_spike_t, np.zeros_like(hom_spike_t), \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More spike train statistics\n",
    "\n",
    "**Q** Here's a slightly harder question about the properties of Poisson processes. Calculate the interspike *intervals* from `spike_t` (hint: look at the documentation for `np.diff`), then plot a histogram. What function does this look like? Calculate the sample mean and variance. How do these relate to the rate of the process? What is the coefficient of variation of the interspike distribution (CV = $\\mu / \\sigma$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inhomogeneous Poisson Processes\n",
    "\n",
    "It's also possible for the rate of a Poisson process to vary in time; that is, for $\\lambda$ to be a function of $t$.\n",
    "\n",
    "$$p(n|\\lambda(t)) = \\frac{\\lambda(t)^n}{n!}\\exp(-\\lambda(t))$$\n",
    "\n",
    "As before, we need to discretize time and determine the probability that there is a spike in some interval $(t, t + \\Delta)$; the only difference is that some intervals are more likely to have spikes than others.\n",
    "\n",
    "We could simulate an inhomogeneous Poisson process in much the same way as we did above, but we need to vary $\\lambda$ in each bin.\n",
    "\n",
    "Let's look at a simple example where the intensity linearly ramps up from zero and then back down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "T     = 100     # s\n",
    "Delta = 0.001   # s\n",
    "N     = int(T / Delta)\n",
    "bins  = np.arange(0, T, Delta)\n",
    "# rate is now a function of time\n",
    "inh_rate  = np.concatenate([np.linspace(0.0, 4.0, N//2),\n",
    "                            np.linspace(4.0, 0.0, N//2)])\n",
    "\n",
    "# generate N values from a uniform distribution\n",
    "rand = dists.uniform().rvs(N)\n",
    "# this is an alternative method of simulating spiking based on the Bernoulli distribution\n",
    "# compare each value to lambda = rate * Delta; if it's greater, then the bin gets a spike\n",
    "lam  = inh_rate * Delta\n",
    "inh_spikes = (inh_rate * Delta) > rand\n",
    "inh_spike_i = np.nonzero(inh_spikes)[0]\n",
    "inh_spike_t = bins[inh_spike_i]\n",
    "# here's one way to plot a raster of spike times\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "axes.plot(bins, inh_rate)\n",
    "axes.plot(inh_spike_t, np.zeros_like(inh_spike_t), \"k|\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Calculate the mean, variance, and CV of the interspike intervals for this spike train (`inh_spike_t`). Is the relationship with the rate the same as you discovered for the homogeneous process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating spike rates\n",
    "\n",
    "Let's think about how we can estimate $\\lambda$ for Poisson processes.\n",
    "\n",
    "If we assume that the process is homogeneous over each trial, then we have a simple observational model where the number of spikes is a random sample from the Poisson distribution.\n",
    "\n",
    "$$p(y_i|\\lambda) = \\frac{\\lambda^n}{n!}\\exp(-\\lambda)$$\n",
    "\n",
    "Given a set of trials, we can estimate $\\lambda$ from the sample mean of the spike count:\n",
    "\n",
    "$$\\hat{\\lambda} = \\sum_i y_i$$\n",
    "\n",
    "**Q**: What is the estimated intensity of the homogeneous spike train we generated above (`hom_spike_t`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem is a lot trickier if the process is inhomogeneous, because now we're trying to estimate a continuous function of time, $\\lambda(t)$.\n",
    "\n",
    "In this setting, people usually talk about rate rather than intensity ($\\lambda$), so we'll use $r(t)$ from here on.\n",
    "\n",
    "The issue we confront is that $r(t)$ is a continuous function. We can discretize it into small intervals of $(t, t + \\Delta)$ and count the number of spike in each interval, but as we make $\\Delta$ smaller to get higher temporal resolution, we reach the point at which each bin has either one or zero spikes, which doesn't tell us much about the rate. We can address this problem by averaging across multiple trials. If we use $\\langle \\rangle$ to denote averaging across trials, this looks like:\n",
    "\n",
    "$$r(t) = \\frac{1}{\\Delta} \\int_t^{t+\\Delta} d\\tau\\; \\langle \\rho(t) \\rangle$$\n",
    "\n",
    "You hopefully can see that as $\\Delta$ gets smaller, the number of trials you need to average to get a smooth function gets larger. So part of our problem is to determine what $\\Delta$ should be. More practically, at what time scale do we think the rate is changing?\n",
    "\n",
    "There are a number of different ways of approximating $r(t)$. We'll look at a couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spike time histogram\n",
    "\n",
    "For historical reasons, this is also called a peri-stimulus spike time histogram (PSTH), even when there isn't a stimulus.\n",
    "\n",
    "The simplest way of approximating the rate is to divide the interval up into a fixed number of bins of duration $\\Delta$ and count how many spikes occurred in each bin. The rate is simply the number of spikes divided by $\\Delta$.\n",
    "\n",
    "The main problem with histograms is that setting the bin size is largely subjective. Try adjusting the bin count (which is just the inverse of the bin size) variable and see what gives you the best tradeoff between variability and temporal resolution.\n",
    "\n",
    "There is still active development of new methods for adaptively setting bin sizes in timing histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "bin_size = 5.0\n",
    "bin_count = widgets.IntSlider(value=int(T / bin_size), min=1, max=100, step=1, \n",
    "                              description=\"bin count:\", continuous_update=True)\n",
    "display(bin_count)\n",
    "\n",
    "ax.plot(bins, inh_rate)\n",
    "ax.plot(inh_spike_t, np.zeros_like(inh_spike_t), \"k|\")\n",
    "r_est, edges  = np.histogram(inh_spike_t, bins=np.arange(0, T + bin_size, bin_size))\n",
    "p = ax.step(edges[1:], r_est / bin_size)\n",
    "\n",
    "def update(bin_count):\n",
    "    bin_size = T / bin_count\n",
    "    r_est, edges  = np.histogram(inh_spike_t, bins=np.arange(0, T + bin_size, bin_size))\n",
    "    print(\"bin size:\", bin_size)\n",
    "    p[0].set_data(edges[1:], r_est / bin_size)\n",
    "    \n",
    "widgets.interactive_output(update, {\"bin_count\": bin_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoothing\n",
    "\n",
    "Another problem with the PSTH is that the count in each bin can depend very strongly on where the edges of the bins are.\n",
    "\n",
    "One solution to this problem is to use a **sliding window**. The simplest window is simply a square function with a defined width and a total area equal to 1.0.\n",
    "\n",
    "For example, here's a 10 ms window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n",
    "window_size = 2.0\n",
    "w_t = np.arange(-10.0, 10.0, Delta)\n",
    "w = np.zeros_like(w_t)\n",
    "w[(-window_size/2 < w_t) & (w_t <window_size/2)] = 1. / window_size\n",
    "ax.plot(w_t, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolution\n",
    "\n",
    "We can express the sliding window operation as a sum of the window function for the values of the spike times.\n",
    "\n",
    "$$r(t) \\approx \\sum_{i=0}^{N-1} w(t - t_i)$$\n",
    "\n",
    "This is equivalent to doing an integral over the response function:\n",
    "\n",
    "$$r(t) \\approx \\int_{-\\infty}^{\\infty} d\\tau\\; w(\\tau) \\rho(t - \\tau)$$\n",
    "\n",
    "This integral is also called a linear **convolution** or filter, and we'll be seeing a lot of them.\n",
    "\n",
    "Numpy has a function that can calculate this convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "r_est = np.convolve(inh_spikes, w, mode='same')\n",
    "ax.plot(bins, inh_rate)\n",
    "ax.plot(inh_spike_t, np.zeros_like(inh_spike_t), \"k|\")\n",
    "ax.plot(bins, r_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use any function as a window as long as it goes to zero outside $\\tau = 0$ and its integral is 1.0.\n",
    "\n",
    "A popular choice is to use a Gaussian, which smooths the function by downweighting points further away from $\\tau = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n",
    "sigma = 2.0\n",
    "w = 1 / np.sqrt(2 * np.pi) / sigma * np.exp(-w_t**2 / 2 / sigma**2)\n",
    "plt.plot(w_t, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "sigma_w = widgets.FloatSlider(value=sigma, min=0.1, max=5.0, step=0.1,\n",
    "                              description=\"sigma:\", continuous_update=False)\n",
    "display(sigma_w)\n",
    "\n",
    "ax.plot(bins, inh_rate)\n",
    "ax.plot(inh_spike_t, np.zeros_like(inh_spike_t), \"k|\")\n",
    "r_est = np.convolve(inh_spikes, w, mode='same')\n",
    "p = ax.plot(bins, r_est)\n",
    "\n",
    "def update(sigma):\n",
    "    w_T = 10\n",
    "    w_t = np.arange(-w_T, w_T, Delta)\n",
    "    w = 1 / np.sqrt(2 * np.pi) / sigma * np.exp(-w_t**2 / 2 / sigma**2)\n",
    "    r_est = np.convolve(inh_spikes, w, mode='same')\n",
    "    p[0].set_data(bins, r_est)\n",
    "    \n",
    "widgets.interactive_output(update, {\"sigma\": sigma_w})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Averaging trials\n",
    "\n",
    "Hopefully, these exercises have illustrated the fundamental tradeoff between variance and temporal resolution. As you increase the bin (or window) size ($\\Delta$), the estimated rate becomes less variable, but the temporal resolution decreases. Thus, smoothing can interfere with detecting rapid changes in the underlying rate function.\n",
    "\n",
    "As noted above, the solution to this problem is to average across trials. In essence, this gives you multiple independent estimates of the rate at any given instant, thereby reducing the amount of smoothing you need.\n",
    "\n",
    "We can represent spiking responses over multiple trials using a two-dimensional array. Here's a simulation of 10 trials to the above rate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're keeping all the parameters the same as above, but resetting the random seed\n",
    "np.random.seed(1)\n",
    "n_trials = 10\n",
    "\n",
    "# it's often a good idea to initialize the array first, then fill it up\n",
    "# recall that N is the number of bins\n",
    "mt_spikes = np.zeros((N, n_trials))\n",
    "# we're going to store the spike times in a ragged array (i.e. a list of arrays)\n",
    "mt_spikes_t = []\n",
    "for trial in range(n_trials):\n",
    "    # generate N values from a uniform distribution\n",
    "    rand = dists.uniform().rvs(N)\n",
    "    lam  = inh_rate * Delta\n",
    "    spikes = (inh_rate * Delta) > rand\n",
    "    idx = np.nonzero(spikes)[0]\n",
    "    mt_spikes[:, trial] = spikes\n",
    "    mt_spikes_t.append(bins[idx])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "axes.plot(bins, inh_rate)    \n",
    "for i, trial in enumerate(mt_spikes_t):\n",
    "    axes.plot(trial, np.ones_like(trial) * i / n_trials, \"k|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The rate function is calculated by averaging smoothed estimates across trials. Compare the plot below to the smoothed estimate based on 1 trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2.0\n",
    "w_T = 10\n",
    "w_t = np.arange(-w_T, w_T, Delta)\n",
    "w = 1 / np.sqrt(2 * np.pi) / sigma * np.exp(-w_t**2 / 2 / sigma**2)\n",
    "r_est = np.convolve(mt_spikes.mean(1), w, mode='same')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "axes.plot(bins, inh_rate)\n",
    "axes.plot(bins, r_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting it together\n",
    "\n",
    "For the rest of the class period, work on the following exercises.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Consider an inhomogeneous Poisson process with a time-varying rate specified by:\n",
    "\n",
    "$$r(t) = 1/4[\\sin(2\\pi\\omega t) + 1]$$\n",
    "\n",
    "Use $\\omega$ = 3 Hz, and a response interval from 0 to 2 s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Plot r(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Generate 20 independently simulated spike trains and plot them as rasters. There is code in previous notebooks you can use to make the raster plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**. Using a bin size of 10 ms, calculate the PSTHs averaged from the first 10 trials and the last 10 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Now simulate 1000 trials and calculate a PSTH from the first and second half. How do these PSTHs relate to $\\lambda(t)$? Do more trials give you a more precise estimate of the rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Calculate a PSTH *for each trial* of the 1000-trial simulation. This will yield a 1000 x 200 array, with trials along one axis and time along the other. For each time bin, calculate and plot the mean, variance, and Fano factor. Each of these will be a 200-element array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Repeat the analysis in the previous question but with a bin size of 150 ms. What happens to the Fano factor? Why is it important to choose a bin size such that the rate is not changing much within each bin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
